#ifndef MODEL_WEIGHTS_H
#define MODEL_WEIGHTS_H

/**
 * @file model_weights.h
 * @brief Quantized model weights for Spectral-Gate TinyML inference
 * 
 * Auto-generated by generate_physics.py on 2026-01-07 21:11:33
 * 
 * Model Architecture:
 *   - Input:  64 spectral features (normalized fixed-point)
 *   - Output: 3 classes (0=normal, 1=anomaly, 2=uncertain)
 *   - Type:   Single-layer perceptron with quantized int8 weights
 * 
 * Quantization:
 *   - Weights: int8 [-128, 127], effective range ~[-1.0, 1.0]
 *   - Biases:  int8 [-128, 127]
 *   - Scale:   Q15.16 fixed-point
 */

#include <cstdint>

// Model dimensions
constexpr size_t MODEL_INPUT_SIZE = 64;
constexpr size_t MODEL_OUTPUT_SIZE = 3;

// Quantization scale factor (Q15.16 format, represents 1.0)
constexpr int32_t MODEL_SCALE_FACTOR = 65536;

// Quantized weights: [3][64] stored row-major
constexpr int8_t MODEL_WEIGHTS[] = {
     -36,  -58,    6,   -2,   -7,  -29,  -38,  -42,   44,  -56,  -57,  -41,   -9,   -5,  -58,  -14,
      43,   -8,   50,    7,  -63,  -24,   44,   23,    7,  -25,   -9,   22,  -38,  -41,   33,  -40,
      27,   24,    3,  -53,   53,  -33,   32,  -44,   11,   28,  -15,  -47,  -53,   -6,   10,  -44,
      -5,  -39,   33,    7,   52,   29,  -23,   30,   26,  -11,    4,  -46,  -21,   -2,  -23,   54,
      33,    5,   -8,   19,  -50,   -6,  -56,   16,   38,    4,  -48,  -10,   16,  -10,   63,   37,
      53,  -28,    3,  -29,   -1,    3,   45,   38,   28,   -8,  -29,   62,  -41,  -52,  -36,  -25,
     -24,   44,  -48,   34,   33,   55,    0,  -62,  -35,    4,   23,  -36,   11,   47,  -24,   52,
     -64,    3,  -19,  -37,   12,  -14,  -25,   31,  -23,  -64,   18,   61,  -60,  -36,   28,   14,
      -3,  -50,   -3,  -44,  -43,   60,  -47,  -32,  -32,   57,  -22,    3,   44,  -10,  -13,   15,
      38,   31,   48,   51,  -34,   -1,   -7,  -48,   22,  -59,   -6,   -8,  -63,  -46,  -49,   -6,
     -47,  -56,   20,  -46,   -4,    7,   60,  -10,  -31,   57,   -2,   57,   40,  -16,  -40,  -40,
      46,   26,   44,   41,   55,  -51,  -39,  -49,   39,   22,  -37,   -1,  -15,  -16,   50,  -29
};

// Quantized biases: [3]
constexpr int8_t MODEL_BIASES[] = {
      22,   -9,    3
};

#endif // MODEL_WEIGHTS_H
