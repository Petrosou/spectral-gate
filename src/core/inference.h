#ifndef INFERENCE_H
#define INFERENCE_H

#include <cstdint>
#include <cstddef>
#include "hal/hal_interface.h"
#include "decision.h"

namespace spectral_gate {
namespace core {

/**
 * @brief Quantized TinyML inference engine
 * 
 * Implements a simple single-layer perceptron using quantized (int8) weights
 * and fixed-point arithmetic for efficient embedded execution.
 */
class InferenceEngine {
public:
    /**
     * @brief Initialize the inference engine with model weights
     * @param weights Pointer to quantized weight array (int8)
     * @param biases Pointer to quantized bias array (int8)
     * @param input_size Number of input features
     * @param output_size Number of output classes
     * @param scale_factor Quantization scale factor (fixed-point)
     */
    InferenceEngine(
        const int8_t* weights,
        const int8_t* biases,
        size_t input_size,
        size_t output_size,
        hal::fixed_t scale_factor
    );

    /**
     * @brief Run inference on spectral features
     * @param features Input feature array (fixed-point)
     * @param num_features Number of features (must match input_size)
     * @return Inference result with confidence and predicted class
     */
    InferenceResult run(const hal::fixed_t* features, size_t num_features);

    /**
     * @brief Get input size expected by the model
     */
    size_t get_input_size() const { return input_size_; }

    /**
     * @brief Get output size (number of classes)
     */
    size_t get_output_size() const { return output_size_; }

private:
    const int8_t* weights_;
    const int8_t* biases_;
    size_t input_size_;
    size_t output_size_;
    hal::fixed_t scale_factor_;

    /**
     * @brief Compute dot product for a single output neuron
     */
    hal::fixed_t dot_product(const hal::fixed_t* input, size_t output_idx);

    /**
     * @brief Apply softmax-like normalization to outputs
     */
    void normalize_outputs(hal::fixed_t* outputs, size_t count);

    /**
     * @brief Find argmax of output array
     */
    uint8_t argmax(const hal::fixed_t* outputs, size_t count);
};

/**
 * @brief Create inference engine with compiled model weights
 * 
 * Uses weights from model_weights.h generated by the Python script
 */
InferenceEngine create_default_engine();

} // namespace core
} // namespace spectral_gate

#endif // INFERENCE_H
